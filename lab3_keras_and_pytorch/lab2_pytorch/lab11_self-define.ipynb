{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Self-Define自定义\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义loss函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方式1：定义为一个类<br>\n",
    "``\n",
    "class TF_Sparse_Softmax_Cross_Entropy_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TF_Sparse_Softmax_Cross_Entropy_Loss, self).__init__()\n",
    "    def forward(self, input, target):\n",
    "        loss_op = -torch.sum((F.log_softmax(input, dim=-1) * target), dim=-1).mean()\n",
    "        return loss_op\n",
    "``\n",
    "<br>\n",
    "方式2：作为网络的一部分<br>\n",
    "``\n",
    "class Dense(nn.Module):\n",
    "    def forward(self, input):\n",
    "            output = torch.matmul(input, self.weight.t()) + self.bias\n",
    "            return output\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义layer\n",
    "以全相连网络为例。\n",
    "在定义矩阵时，将矩阵的大小逆序。因为这些网络的矩阵需要用Parameter包装，通过转置的方式访问。\n",
    "\n",
    "``\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Dense, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.zeros(out_features, in_features))\n",
    "        self.bias = Parameter(torch.zeros(out_features))\n",
    "    def forward(self, input):\n",
    "        output = torch.matmul(input, self.weight.t()) + self.bias\n",
    "        return output\n",
    "``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义 optimizer\n",
    "Pytorch中自定义优化器比较复杂，在此不多赘述。感兴趣的读者可参考源码，仿写。\n",
    "\n",
    "类似的，Pytorch也能像TensorFlow一样改变梯度。\n",
    "### 手动完成梯度传播\n",
    "``optimizer.step()``等价于<brr>\n",
    "``\n",
    "for f in model.parameters():\n",
    "    torch.nn.utils.clip_grad_norm()\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available? True\n",
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Dense()\n",
      ")\n",
      "loss: 2.30258, acc: 0.09700\n",
      "loss: 2.29881, acc: 0.28000\n",
      "loss: 2.29455, acc: 0.46900\n",
      "loss: 2.29054, acc: 0.51700\n",
      "loss: 2.28744, acc: 0.39200\n",
      "loss: 2.28173, acc: 0.57400\n",
      "loss: 2.27772, acc: 0.52000\n",
      "loss: 2.27353, acc: 0.44700\n",
      "loss: 2.26831, acc: 0.49300\n",
      "loss: 2.26301, acc: 0.44300\n",
      "loss: 2.25822, acc: 0.43300\n",
      "loss: 2.25360, acc: 0.48400\n",
      "loss: 2.25045, acc: 0.50500\n",
      "loss: 2.24293, acc: 0.58500\n",
      "loss: 2.23779, acc: 0.59300\n",
      "loss: 2.22530, acc: 0.62600\n",
      "loss: 2.22009, acc: 0.58400\n",
      "loss: 2.21340, acc: 0.62000\n",
      "loss: 2.20444, acc: 0.63200\n",
      "loss: 2.19184, acc: 0.63900\n",
      "loss: 2.19070, acc: 0.61100\n",
      "loss: 2.17900, acc: 0.58100\n",
      "loss: 2.16739, acc: 0.59500\n",
      "loss: 2.15500, acc: 0.63600\n",
      "loss: 2.13929, acc: 0.65100\n",
      "loss: 2.12724, acc: 0.65300\n",
      "loss: 2.11739, acc: 0.62300\n",
      "loss: 2.09119, acc: 0.65500\n",
      "loss: 2.08123, acc: 0.63800\n",
      "loss: 2.07224, acc: 0.61700\n",
      "loss: 2.05157, acc: 0.65500\n",
      "loss: 2.01760, acc: 0.66300\n",
      "loss: 2.01729, acc: 0.63300\n",
      "loss: 1.99024, acc: 0.66000\n",
      "loss: 1.96957, acc: 0.65400\n",
      "loss: 1.94963, acc: 0.66700\n",
      "loss: 1.91968, acc: 0.70200\n",
      "loss: 1.90373, acc: 0.65900\n",
      "loss: 1.86614, acc: 0.68800\n",
      "loss: 1.85376, acc: 0.68000\n",
      "loss: 1.81434, acc: 0.70800\n",
      "loss: 1.80700, acc: 0.65800\n",
      "loss: 1.74681, acc: 0.70800\n",
      "loss: 1.72160, acc: 0.69000\n",
      "loss: 1.72833, acc: 0.68600\n",
      "loss: 1.67328, acc: 0.69700\n",
      "loss: 1.64236, acc: 0.70800\n",
      "loss: 1.62294, acc: 0.70800\n",
      "loss: 1.57131, acc: 0.72900\n",
      "loss: 1.55943, acc: 0.72100\n",
      "loss: 1.55599, acc: 0.70000\n",
      "loss: 1.49500, acc: 0.72800\n",
      "loss: 1.48698, acc: 0.72300\n",
      "loss: 1.46550, acc: 0.71600\n",
      "loss: 1.44761, acc: 0.71600\n",
      "loss: 1.39791, acc: 0.73100\n",
      "loss: 1.37439, acc: 0.73400\n",
      "loss: 1.39537, acc: 0.71100\n",
      "loss: 1.31064, acc: 0.75900\n",
      "loss: 1.30755, acc: 0.75900\n",
      "loss: 1.27233, acc: 0.75600\n",
      "loss: 1.23891, acc: 0.76700\n",
      "loss: 1.25049, acc: 0.75200\n",
      "loss: 1.19918, acc: 0.77000\n",
      "loss: 1.20216, acc: 0.75400\n",
      "loss: 1.15634, acc: 0.77300\n",
      "loss: 1.15399, acc: 0.75800\n",
      "loss: 1.11892, acc: 0.75900\n",
      "loss: 1.11142, acc: 0.77700\n",
      "loss: 1.07304, acc: 0.77900\n",
      "loss: 1.06618, acc: 0.78200\n",
      "loss: 1.09100, acc: 0.75400\n",
      "loss: 1.03871, acc: 0.78800\n",
      "loss: 0.97925, acc: 0.83000\n",
      "loss: 1.01733, acc: 0.79500\n",
      "loss: 0.95855, acc: 0.80700\n",
      "loss: 0.97518, acc: 0.78500\n",
      "loss: 0.98050, acc: 0.79400\n",
      "loss: 1.00273, acc: 0.77700\n",
      "loss: 0.91690, acc: 0.82700\n",
      "loss: 0.91290, acc: 0.80700\n",
      "loss: 0.90310, acc: 0.79900\n",
      "loss: 0.88739, acc: 0.80400\n",
      "loss: 0.90318, acc: 0.80300\n",
      "loss: 0.91333, acc: 0.80800\n",
      "loss: 0.90866, acc: 0.79900\n",
      "loss: 0.88819, acc: 0.80800\n",
      "loss: 0.82230, acc: 0.82600\n",
      "loss: 0.84767, acc: 0.80900\n",
      "loss: 0.84307, acc: 0.80100\n",
      "loss: 0.82413, acc: 0.82400\n",
      "loss: 0.82634, acc: 0.80200\n",
      "loss: 0.78041, acc: 0.82800\n",
      "loss: 0.80797, acc: 0.80200\n",
      "loss: 0.76616, acc: 0.81700\n",
      "loss: 0.77719, acc: 0.83100\n",
      "loss: 0.80036, acc: 0.81000\n",
      "loss: 0.76181, acc: 0.83900\n",
      "loss: 0.75687, acc: 0.82500\n",
      "loss: 0.73244, acc: 0.82500\n",
      "loss: 0.74115, acc: 0.83500\n",
      "loss: 0.77066, acc: 0.81400\n",
      "loss: 0.72893, acc: 0.83800\n",
      "loss: 0.70407, acc: 0.82200\n",
      "loss: 0.70852, acc: 0.83700\n",
      "loss: 0.67593, acc: 0.84800\n",
      "loss: 0.70657, acc: 0.83200\n",
      "loss: 0.66892, acc: 0.83800\n",
      "loss: 0.67007, acc: 0.86600\n",
      "loss: 0.66694, acc: 0.85000\n",
      "loss: 0.68964, acc: 0.84600\n",
      "loss: 0.71283, acc: 0.83900\n",
      "loss: 0.63992, acc: 0.84900\n",
      "loss: 0.66746, acc: 0.83800\n",
      "loss: 0.68446, acc: 0.83700\n",
      "loss: 0.62822, acc: 0.86600\n",
      "loss: 0.66286, acc: 0.84400\n",
      "loss: 0.62841, acc: 0.86000\n",
      "loss: 0.64989, acc: 0.86000\n",
      "loss: 0.60998, acc: 0.85800\n",
      "loss: 0.62571, acc: 0.84900\n",
      "loss: 0.63219, acc: 0.83200\n",
      "loss: 0.62710, acc: 0.83000\n",
      "loss: 0.60507, acc: 0.85800\n",
      "loss: 0.59879, acc: 0.85400\n",
      "loss: 0.61069, acc: 0.83600\n",
      "loss: 0.63412, acc: 0.84700\n",
      "loss: 0.59146, acc: 0.85200\n",
      "loss: 0.59827, acc: 0.87200\n",
      "loss: 0.65731, acc: 0.83500\n",
      "loss: 0.58256, acc: 0.87000\n",
      "loss: 0.61598, acc: 0.84200\n",
      "loss: 0.60317, acc: 0.84500\n",
      "loss: 0.55960, acc: 0.86000\n",
      "loss: 0.59308, acc: 0.83900\n",
      "loss: 0.60275, acc: 0.83700\n",
      "loss: 0.58082, acc: 0.86300\n",
      "loss: 0.63235, acc: 0.83200\n",
      "loss: 0.59500, acc: 0.83600\n",
      "loss: 0.58434, acc: 0.85600\n",
      "loss: 0.61314, acc: 0.84300\n",
      "loss: 0.57711, acc: 0.85800\n",
      "loss: 0.52842, acc: 0.88100\n",
      "loss: 0.53655, acc: 0.87300\n",
      "loss: 0.55053, acc: 0.86300\n",
      "loss: 0.54109, acc: 0.85900\n",
      "loss: 0.55512, acc: 0.86100\n",
      "loss: 0.55050, acc: 0.85600\n",
      "loss: 0.52983, acc: 0.86700\n",
      "loss: 0.52364, acc: 0.86800\n",
      "loss: 0.52264, acc: 0.87100\n",
      "loss: 0.54065, acc: 0.86100\n",
      "loss: 0.56658, acc: 0.85800\n",
      "loss: 0.52296, acc: 0.86900\n",
      "loss: 0.56121, acc: 0.85800\n",
      "loss: 0.48236, acc: 0.88700\n",
      "loss: 0.55126, acc: 0.85500\n",
      "loss: 0.49850, acc: 0.87100\n",
      "loss: 0.50335, acc: 0.87700\n",
      "loss: 0.50753, acc: 0.88400\n",
      "loss: 0.55560, acc: 0.85300\n",
      "loss: 0.49980, acc: 0.87300\n",
      "loss: 0.51111, acc: 0.86500\n",
      "loss: 0.51093, acc: 0.86700\n",
      "loss: 0.53169, acc: 0.86400\n",
      "loss: 0.51245, acc: 0.86300\n",
      "loss: 0.50755, acc: 0.87600\n",
      "loss: 0.54874, acc: 0.86400\n",
      "loss: 0.53385, acc: 0.86000\n",
      "loss: 0.50197, acc: 0.87500\n",
      "loss: 0.50657, acc: 0.87200\n",
      "loss: 0.45587, acc: 0.89700\n",
      "loss: 0.45030, acc: 0.88700\n",
      "loss: 0.48963, acc: 0.88000\n",
      "loss: 0.51461, acc: 0.85900\n",
      "loss: 0.50210, acc: 0.87000\n",
      "loss: 0.48112, acc: 0.87100\n",
      "loss: 0.50094, acc: 0.87600\n",
      "loss: 0.44692, acc: 0.88900\n",
      "loss: 0.48176, acc: 0.89100\n",
      "loss: 0.45697, acc: 0.88100\n",
      "loss: 0.49336, acc: 0.87200\n",
      "loss: 0.54216, acc: 0.84400\n",
      "loss: 0.48288, acc: 0.88100\n",
      "loss: 0.51640, acc: 0.85300\n",
      "loss: 0.48493, acc: 0.88000\n",
      "loss: 0.45951, acc: 0.87900\n",
      "loss: 0.47632, acc: 0.87300\n",
      "loss: 0.48727, acc: 0.88800\n",
      "loss: 0.47915, acc: 0.88000\n",
      "loss: 0.50970, acc: 0.87400\n",
      "loss: 0.47482, acc: 0.86700\n",
      "loss: 0.45313, acc: 0.88300\n",
      "loss: 0.49113, acc: 0.87200\n",
      "loss: 0.46051, acc: 0.88200\n",
      "loss: 0.46849, acc: 0.89000\n",
      "loss: 0.46813, acc: 0.87400\n",
      "loss: 0.46316, acc: 0.88000\n",
      "loss: 0.44017, acc: 0.89100\n",
      "loss: 0.43884, acc: 0.88400\n",
      "loss: 0.43599, acc: 0.88300\n",
      "loss: 0.41770, acc: 0.89800\n",
      "loss: 0.40142, acc: 0.90100\n",
      "loss: 0.42785, acc: 0.88200\n",
      "loss: 0.43264, acc: 0.87900\n",
      "loss: 0.42844, acc: 0.88900\n",
      "loss: 0.45676, acc: 0.88500\n",
      "loss: 0.47857, acc: 0.87100\n",
      "loss: 0.45555, acc: 0.87700\n",
      "loss: 0.41781, acc: 0.89100\n",
      "loss: 0.45504, acc: 0.87600\n",
      "loss: 0.42475, acc: 0.88500\n",
      "loss: 0.43013, acc: 0.89900\n",
      "loss: 0.42686, acc: 0.89600\n",
      "loss: 0.43640, acc: 0.87500\n",
      "loss: 0.46483, acc: 0.87900\n",
      "loss: 0.43831, acc: 0.88100\n",
      "loss: 0.48477, acc: 0.88100\n",
      "loss: 0.40876, acc: 0.89000\n",
      "loss: 0.48315, acc: 0.86400\n",
      "loss: 0.45664, acc: 0.87600\n",
      "loss: 0.42418, acc: 0.88300\n",
      "loss: 0.45584, acc: 0.87300\n",
      "loss: 0.41262, acc: 0.89800\n",
      "loss: 0.43764, acc: 0.87600\n",
      "loss: 0.42947, acc: 0.88200\n",
      "loss: 0.44111, acc: 0.86900\n",
      "loss: 0.40795, acc: 0.89500\n",
      "loss: 0.47879, acc: 0.87100\n",
      "loss: 0.41989, acc: 0.89500\n",
      "loss: 0.44641, acc: 0.87500\n",
      "loss: 0.40193, acc: 0.89600\n",
      "loss: 0.43694, acc: 0.89200\n",
      "loss: 0.42655, acc: 0.88600\n",
      "loss: 0.47704, acc: 0.86900\n",
      "loss: 0.42020, acc: 0.88300\n",
      "loss: 0.42459, acc: 0.88800\n",
      "loss: 0.42606, acc: 0.88400\n",
      "loss: 0.40663, acc: 0.89300\n",
      "loss: 0.38963, acc: 0.89400\n",
      "loss: 0.40648, acc: 0.90300\n",
      "loss: 0.41211, acc: 0.89200\n",
      "loss: 0.43305, acc: 0.89000\n",
      "loss: 0.42049, acc: 0.87700\n",
      "loss: 0.46686, acc: 0.88100\n",
      "loss: 0.43555, acc: 0.87200\n",
      "loss: 0.43872, acc: 0.88100\n",
      "loss: 0.40519, acc: 0.89900\n",
      "loss: 0.43156, acc: 0.88800\n",
      "loss: 0.44698, acc: 0.87600\n",
      "loss: 0.39552, acc: 0.89600\n",
      "loss: 0.39850, acc: 0.89400\n",
      "loss: 0.42985, acc: 0.88800\n",
      "loss: 0.43631, acc: 0.87700\n",
      "loss: 0.39890, acc: 0.88200\n",
      "loss: 0.39899, acc: 0.88200\n",
      "loss: 0.38287, acc: 0.90000\n",
      "loss: 0.45664, acc: 0.87700\n",
      "loss: 0.40554, acc: 0.88700\n",
      "loss: 0.38736, acc: 0.88800\n",
      "loss: 0.41035, acc: 0.88100\n",
      "loss: 0.44209, acc: 0.86400\n",
      "loss: 0.38133, acc: 0.89100\n",
      "loss: 0.38174, acc: 0.90900\n",
      "loss: 0.42472, acc: 0.89200\n",
      "loss: 0.40731, acc: 0.88200\n",
      "loss: 0.37679, acc: 0.90200\n",
      "loss: 0.40771, acc: 0.87700\n",
      "loss: 0.41535, acc: 0.89700\n",
      "loss: 0.37046, acc: 0.90700\n",
      "loss: 0.38430, acc: 0.89500\n",
      "loss: 0.37838, acc: 0.90200\n",
      "loss: 0.38241, acc: 0.89500\n",
      "loss: 0.40967, acc: 0.88100\n",
      "loss: 0.38868, acc: 0.88700\n",
      "loss: 0.38511, acc: 0.88500\n",
      "loss: 0.36957, acc: 0.89700\n",
      "loss: 0.37275, acc: 0.88900\n",
      "loss: 0.36034, acc: 0.91100\n",
      "loss: 0.37594, acc: 0.90600\n",
      "loss: 0.39341, acc: 0.88800\n",
      "loss: 0.37037, acc: 0.89800\n",
      "loss: 0.39789, acc: 0.89700\n",
      "loss: 0.40595, acc: 0.88900\n",
      "loss: 0.40542, acc: 0.88900\n",
      "loss: 0.41630, acc: 0.88900\n",
      "loss: 0.43972, acc: 0.88100\n",
      "loss: 0.36931, acc: 0.90000\n",
      "loss: 0.39725, acc: 0.89100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.37768, acc: 0.90100\n",
      "loss: 0.37709, acc: 0.90700\n",
      "loss: 0.38232, acc: 0.89300\n",
      "loss: 0.38241, acc: 0.88600\n",
      "loss: 0.41691, acc: 0.89500\n",
      "loss: 0.36571, acc: 0.89200\n",
      "loss: 0.37619, acc: 0.90800\n",
      "loss: 0.36589, acc: 0.89300\n",
      "loss: 0.38841, acc: 0.89300\n",
      "loss: 0.41773, acc: 0.88500\n",
      "loss: 0.42625, acc: 0.88100\n",
      "loss: 0.39270, acc: 0.90400\n",
      "loss: 0.38448, acc: 0.89000\n",
      "loss: 0.39911, acc: 0.89700\n",
      "loss: 0.40198, acc: 0.88500\n",
      "loss: 0.35688, acc: 0.89200\n",
      "loss: 0.38786, acc: 0.89100\n",
      "loss: 0.38931, acc: 0.90100\n",
      "loss: 0.36096, acc: 0.91100\n",
      "loss: 0.40215, acc: 0.87900\n",
      "loss: 0.34246, acc: 0.90300\n",
      "loss: 0.41366, acc: 0.89100\n",
      "loss: 0.36434, acc: 0.89200\n",
      "loss: 0.38404, acc: 0.89500\n",
      "loss: 0.41210, acc: 0.88200\n",
      "loss: 0.33576, acc: 0.91800\n",
      "loss: 0.33685, acc: 0.90700\n",
      "loss: 0.37144, acc: 0.89900\n",
      "loss: 0.34255, acc: 0.90400\n",
      "loss: 0.37646, acc: 0.89400\n",
      "loss: 0.39190, acc: 0.88400\n",
      "loss: 0.39348, acc: 0.88100\n",
      "loss: 0.38304, acc: 0.90500\n",
      "loss: 0.35244, acc: 0.89800\n",
      "loss: 0.39884, acc: 0.88000\n",
      "loss: 0.41543, acc: 0.88100\n",
      "loss: 0.36884, acc: 0.89800\n",
      "loss: 0.33040, acc: 0.91400\n",
      "loss: 0.38367, acc: 0.89400\n",
      "loss: 0.38329, acc: 0.89400\n",
      "loss: 0.36076, acc: 0.90300\n",
      "loss: 0.39193, acc: 0.89500\n",
      "loss: 0.39082, acc: 0.89200\n",
      "loss: 0.37848, acc: 0.89500\n",
      "loss: 0.35130, acc: 0.90000\n",
      "loss: 0.37272, acc: 0.89500\n",
      "loss: 0.36844, acc: 0.91300\n",
      "loss: 0.36815, acc: 0.90500\n",
      "loss: 0.36538, acc: 0.91100\n",
      "loss: 0.30185, acc: 0.91600\n",
      "loss: 0.38412, acc: 0.89300\n",
      "loss: 0.39328, acc: 0.89500\n",
      "loss: 0.35710, acc: 0.89000\n",
      "loss: 0.35778, acc: 0.90100\n",
      "loss: 0.38123, acc: 0.89200\n",
      "loss: 0.36849, acc: 0.89700\n",
      "loss: 0.32683, acc: 0.91100\n",
      "loss: 0.33025, acc: 0.90700\n",
      "loss: 0.38401, acc: 0.89300\n",
      "loss: 0.36741, acc: 0.89800\n",
      "loss: 0.37841, acc: 0.89300\n",
      "loss: 0.37957, acc: 0.90600\n",
      "loss: 0.35756, acc: 0.90000\n",
      "loss: 0.37903, acc: 0.88400\n",
      "loss: 0.38343, acc: 0.89300\n",
      "loss: 0.35929, acc: 0.90600\n",
      "loss: 0.43875, acc: 0.88500\n",
      "loss: 0.34878, acc: 0.90800\n",
      "loss: 0.36104, acc: 0.89100\n",
      "loss: 0.38711, acc: 0.89000\n",
      "loss: 0.30709, acc: 0.91400\n",
      "loss: 0.39134, acc: 0.88500\n",
      "loss: 0.37319, acc: 0.89200\n",
      "loss: 0.31909, acc: 0.92200\n",
      "loss: 0.40971, acc: 0.89200\n",
      "loss: 0.38289, acc: 0.87800\n",
      "loss: 0.38018, acc: 0.89700\n",
      "loss: 0.43079, acc: 0.87900\n",
      "loss: 0.40100, acc: 0.89400\n",
      "loss: 0.31816, acc: 0.90800\n",
      "loss: 0.35608, acc: 0.91500\n",
      "loss: 0.38603, acc: 0.89400\n",
      "loss: 0.35907, acc: 0.90200\n",
      "loss: 0.32606, acc: 0.91100\n",
      "loss: 0.37268, acc: 0.89700\n",
      "loss: 0.33384, acc: 0.89700\n",
      "loss: 0.36159, acc: 0.89000\n",
      "loss: 0.34170, acc: 0.90400\n",
      "loss: 0.30413, acc: 0.90000\n",
      "loss: 0.33459, acc: 0.90800\n",
      "loss: 0.36340, acc: 0.90400\n",
      "loss: 0.32399, acc: 0.91000\n",
      "loss: 0.34003, acc: 0.90400\n",
      "loss: 0.34597, acc: 0.91400\n",
      "loss: 0.36100, acc: 0.89800\n",
      "loss: 0.39279, acc: 0.89100\n",
      "loss: 0.31207, acc: 0.91300\n",
      "loss: 0.39729, acc: 0.89100\n",
      "loss: 0.34660, acc: 0.90200\n",
      "loss: 0.33827, acc: 0.89900\n",
      "loss: 0.40563, acc: 0.88100\n",
      "loss: 0.32615, acc: 0.91200\n",
      "loss: 0.35500, acc: 0.90500\n",
      "loss: 0.35077, acc: 0.90200\n",
      "loss: 0.32739, acc: 0.90300\n",
      "loss: 0.29285, acc: 0.91000\n",
      "loss: 0.35394, acc: 0.90200\n",
      "loss: 0.36110, acc: 0.89900\n",
      "loss: 0.34161, acc: 0.90600\n",
      "loss: 0.35670, acc: 0.90000\n",
      "loss: 0.35646, acc: 0.90100\n",
      "loss: 0.33595, acc: 0.90200\n",
      "loss: 0.34684, acc: 0.89900\n",
      "loss: 0.34241, acc: 0.90300\n",
      "loss: 0.31723, acc: 0.90900\n",
      "loss: 0.35561, acc: 0.89700\n",
      "loss: 0.34555, acc: 0.89800\n",
      "loss: 0.37256, acc: 0.89900\n",
      "loss: 0.35611, acc: 0.90700\n",
      "loss: 0.35417, acc: 0.90600\n",
      "loss: 0.32357, acc: 0.90800\n",
      "loss: 0.33529, acc: 0.91000\n",
      "loss: 0.35844, acc: 0.88900\n",
      "loss: 0.33594, acc: 0.89700\n",
      "loss: 0.34962, acc: 0.90200\n",
      "loss: 0.32799, acc: 0.90800\n",
      "loss: 0.36533, acc: 0.90400\n",
      "loss: 0.35628, acc: 0.90100\n",
      "loss: 0.35070, acc: 0.90600\n",
      "loss: 0.36029, acc: 0.90100\n",
      "loss: 0.34415, acc: 0.90200\n",
      "loss: 0.38108, acc: 0.90000\n",
      "loss: 0.35250, acc: 0.90200\n",
      "loss: 0.34127, acc: 0.89900\n",
      "loss: 0.33221, acc: 0.90900\n",
      "loss: 0.35230, acc: 0.90500\n",
      "loss: 0.38021, acc: 0.88000\n",
      "loss: 0.33958, acc: 0.89700\n",
      "loss: 0.34101, acc: 0.89900\n",
      "loss: 0.33342, acc: 0.89600\n",
      "loss: 0.33124, acc: 0.90900\n",
      "loss: 0.31465, acc: 0.90800\n",
      "loss: 0.29680, acc: 0.92200\n",
      "loss: 0.33943, acc: 0.91100\n",
      "loss: 0.37050, acc: 0.90700\n",
      "loss: 0.34161, acc: 0.90400\n",
      "loss: 0.34917, acc: 0.90500\n",
      "loss: 0.32280, acc: 0.89800\n",
      "loss: 0.33647, acc: 0.91300\n",
      "loss: 0.33489, acc: 0.90700\n",
      "loss: 0.31681, acc: 0.90600\n",
      "loss: 0.35134, acc: 0.90400\n",
      "loss: 0.30219, acc: 0.91500\n",
      "loss: 0.32578, acc: 0.90400\n",
      "loss: 0.37152, acc: 0.90100\n",
      "loss: 0.34485, acc: 0.90900\n",
      "loss: 0.32083, acc: 0.90700\n",
      "loss: 0.31882, acc: 0.91200\n",
      "loss: 0.34629, acc: 0.89900\n",
      "loss: 0.33424, acc: 0.90600\n",
      "loss: 0.31835, acc: 0.91100\n",
      "loss: 0.31664, acc: 0.92500\n",
      "loss: 0.35153, acc: 0.90800\n",
      "loss: 0.34199, acc: 0.89100\n",
      "loss: 0.34944, acc: 0.90800\n",
      "loss: 0.33045, acc: 0.90900\n",
      "loss: 0.33371, acc: 0.90100\n",
      "loss: 0.33530, acc: 0.90500\n",
      "loss: 0.35782, acc: 0.89600\n",
      "loss: 0.31597, acc: 0.90700\n",
      "loss: 0.32360, acc: 0.90700\n",
      "loss: 0.35680, acc: 0.89800\n",
      "loss: 0.36823, acc: 0.89400\n",
      "loss: 0.33759, acc: 0.91600\n",
      "loss: 0.35568, acc: 0.89700\n",
      "loss: 0.33348, acc: 0.91100\n",
      "loss: 0.35465, acc: 0.89400\n",
      "loss: 0.31026, acc: 0.91700\n",
      "loss: 0.32780, acc: 0.90500\n",
      "loss: 0.34645, acc: 0.89700\n",
      "loss: 0.32970, acc: 0.91300\n",
      "loss: 0.34165, acc: 0.91300\n",
      "loss: 0.29908, acc: 0.91000\n",
      "loss: 0.31392, acc: 0.91000\n",
      "loss: 0.34705, acc: 0.90100\n",
      "loss: 0.33224, acc: 0.90500\n",
      "loss: 0.36577, acc: 0.90300\n",
      "loss: 0.32807, acc: 0.91200\n",
      "loss: 0.37436, acc: 0.89500\n",
      "loss: 0.35274, acc: 0.89800\n",
      "loss: 0.28644, acc: 0.92400\n",
      "loss: 0.29725, acc: 0.91800\n",
      "loss: 0.35916, acc: 0.89500\n",
      "loss: 0.33600, acc: 0.89900\n",
      "loss: 0.32665, acc: 0.90900\n",
      "loss: 0.30497, acc: 0.91100\n",
      "loss: 0.34142, acc: 0.90700\n",
      "loss: 0.28967, acc: 0.92000\n",
      "loss: 0.33203, acc: 0.89400\n",
      "loss: 0.34106, acc: 0.89800\n",
      "loss: 0.31338, acc: 0.91500\n",
      "loss: 0.32164, acc: 0.91200\n",
      "loss: 0.34493, acc: 0.91200\n",
      "loss: 0.33041, acc: 0.90300\n",
      "loss: 0.32921, acc: 0.90800\n",
      "loss: 0.30955, acc: 0.91700\n",
      "loss: 0.39988, acc: 0.89500\n",
      "loss: 0.31636, acc: 0.90200\n",
      "loss: 0.31144, acc: 0.90700\n",
      "loss: 0.29980, acc: 0.90000\n",
      "loss: 0.29078, acc: 0.92400\n",
      "loss: 0.31310, acc: 0.92500\n",
      "loss: 0.31873, acc: 0.90800\n",
      "loss: 0.31330, acc: 0.90900\n",
      "loss: 0.36443, acc: 0.90300\n",
      "loss: 0.30028, acc: 0.92000\n",
      "loss: 0.35449, acc: 0.90200\n",
      "loss: 0.39426, acc: 0.88900\n",
      "loss: 0.26663, acc: 0.92500\n",
      "loss: 0.29626, acc: 0.91600\n",
      "loss: 0.34761, acc: 0.89400\n",
      "loss: 0.29829, acc: 0.91700\n",
      "loss: 0.31432, acc: 0.91200\n",
      "loss: 0.36676, acc: 0.89100\n",
      "loss: 0.39519, acc: 0.88500\n",
      "loss: 0.34851, acc: 0.91000\n",
      "loss: 0.31547, acc: 0.90900\n",
      "loss: 0.30698, acc: 0.92200\n",
      "loss: 0.34508, acc: 0.89900\n",
      "loss: 0.36030, acc: 0.90300\n",
      "loss: 0.30379, acc: 0.92200\n",
      "loss: 0.28484, acc: 0.92200\n",
      "loss: 0.27900, acc: 0.91800\n",
      "loss: 0.32695, acc: 0.91200\n",
      "loss: 0.32614, acc: 0.90600\n",
      "loss: 0.30779, acc: 0.90900\n",
      "loss: 0.32530, acc: 0.91500\n",
      "loss: 0.34135, acc: 0.88800\n",
      "loss: 0.32962, acc: 0.90300\n",
      "loss: 0.30261, acc: 0.90700\n",
      "loss: 0.31137, acc: 0.91700\n",
      "loss: 0.29843, acc: 0.91400\n",
      "loss: 0.29931, acc: 0.90900\n",
      "loss: 0.37385, acc: 0.89100\n",
      "loss: 0.32633, acc: 0.90900\n",
      "loss: 0.33728, acc: 0.90500\n",
      "loss: 0.32346, acc: 0.90700\n",
      "loss: 0.30283, acc: 0.92400\n",
      "loss: 0.37475, acc: 0.89600\n",
      "loss: 0.30111, acc: 0.91800\n",
      "loss: 0.35497, acc: 0.89800\n",
      "loss: 0.31032, acc: 0.90400\n",
      "loss: 0.33393, acc: 0.91300\n",
      "loss: 0.34472, acc: 0.89600\n",
      "loss: 0.31588, acc: 0.91300\n",
      "loss: 0.38426, acc: 0.89100\n",
      "loss: 0.36658, acc: 0.89800\n",
      "loss: 0.29406, acc: 0.92400\n",
      "loss: 0.34169, acc: 0.89700\n",
      "loss: 0.31297, acc: 0.91200\n",
      "loss: 0.31099, acc: 0.91000\n",
      "loss: 0.30869, acc: 0.91500\n",
      "loss: 0.27346, acc: 0.91900\n",
      "loss: 0.28248, acc: 0.92300\n",
      "loss: 0.26445, acc: 0.91400\n",
      "loss: 0.29322, acc: 0.91500\n",
      "loss: 0.33006, acc: 0.91300\n",
      "loss: 0.32923, acc: 0.91000\n",
      "loss: 0.33095, acc: 0.90100\n",
      "loss: 0.28539, acc: 0.92300\n",
      "loss: 0.35816, acc: 0.90100\n",
      "loss: 0.32781, acc: 0.90100\n",
      "loss: 0.33130, acc: 0.90800\n",
      "loss: 0.30428, acc: 0.91800\n",
      "loss: 0.34003, acc: 0.91200\n",
      "loss: 0.32347, acc: 0.90500\n",
      "loss: 0.35070, acc: 0.90100\n",
      "loss: 0.32462, acc: 0.90200\n",
      "loss: 0.33170, acc: 0.91000\n",
      "loss: 0.31820, acc: 0.92200\n",
      "loss: 0.33348, acc: 0.90700\n",
      "loss: 0.29666, acc: 0.91300\n",
      "loss: 0.35508, acc: 0.90500\n",
      "loss: 0.28731, acc: 0.90900\n",
      "loss: 0.27361, acc: 0.92600\n",
      "loss: 0.32987, acc: 0.91100\n",
      "loss: 0.31082, acc: 0.92800\n",
      "loss: 0.34618, acc: 0.90600\n",
      "loss: 0.30704, acc: 0.91600\n",
      "loss: 0.31910, acc: 0.90400\n",
      "loss: 0.29518, acc: 0.91600\n",
      "loss: 0.28211, acc: 0.91800\n",
      "loss: 0.25670, acc: 0.92700\n",
      "loss: 0.30740, acc: 0.91000\n",
      "loss: 0.37158, acc: 0.89700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.30637, acc: 0.90400\n",
      "loss: 0.29963, acc: 0.91500\n",
      "loss: 0.26317, acc: 0.91400\n",
      "loss: 0.28652, acc: 0.91000\n",
      "loss: 0.32813, acc: 0.90800\n",
      "loss: 0.31855, acc: 0.90500\n",
      "loss: 0.26804, acc: 0.91900\n",
      "loss: 0.31331, acc: 0.91000\n",
      "loss: 0.30934, acc: 0.90900\n",
      "loss: 0.33890, acc: 0.91300\n",
      "loss: 0.34850, acc: 0.89800\n",
      "loss: 0.28059, acc: 0.90500\n",
      "loss: 0.30741, acc: 0.91200\n",
      "loss: 0.31008, acc: 0.91600\n",
      "loss: 0.33360, acc: 0.90700\n",
      "loss: 0.32155, acc: 0.91100\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "# hyper-parameter\n",
    "training_epoch = 100\n",
    "num_classes = 10\n",
    "learning_rate = 1e-1\n",
    "batch_size = 1000\n",
    "\n",
    "# Mnist digits dataset\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='data/',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='data/',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Dense, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.weight = Parameter(torch.zeros(out_features, in_features))\n",
    "        self.bias = Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = torch.matmul(input, self.weight.t()) + self.bias\n",
    "        return output\n",
    "\n",
    "\n",
    "# 创建网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        # self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.fc2 = Dense(in_features=256, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    def get_loss(self, input, target):\n",
    "        loss_op = -torch.sum((F.log_softmax(input, dim=-1) * target), dim=-1).mean()\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "class TF_Sparse_Softmax_Cross_Entropy_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TF_Sparse_Softmax_Cross_Entropy_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        loss_op = -torch.sum((F.log_softmax(input, dim=-1) * target), dim=-1).mean()\n",
    "        return loss_op\n",
    "\n",
    "\n",
    "# 查看Pytorch是否支持GPU\n",
    "GPU_FLAG = torch.cuda.is_available()\n",
    "print('CUDA available?', GPU_FLAG)\n",
    "# 将模型的参数送到GPU中\n",
    "if GPU_FLAG == True:\n",
    "    model = Net().cuda()\n",
    "    # 定义loss函数\n",
    "    # criterion = nn.CrossEntropyLoss().cuda()\n",
    "    criterion = TF_Sparse_Softmax_Cross_Entropy_Loss().cuda()\n",
    "print(model)  # 输出模型结构\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# one_hot_encoding\n",
    "def one_hot(x, num_classes=10):\n",
    "    x_one_hot = torch.LongTensor(x).view(-1, 1)\n",
    "    x_one_hot = torch.zeros(x_one_hot.size(0), num_classes).scatter_(1, x_one_hot, 1)\n",
    "    return x_one_hot\n",
    "\n",
    "\n",
    "for i in xrange(10):\n",
    "    for _, (batch_images, batch_labels) in enumerate(train_loader):\n",
    "        batch_images = Variable(batch_images).view(-1, 28 ** 2).cuda()\n",
    "        batch_labels_ont_hot = Variable(one_hot(batch_labels)).cuda()\n",
    "        batch_labels = Variable(batch_labels).cuda()\n",
    "\n",
    "        # forward\n",
    "        batch_logits = model(batch_images)\n",
    "        # compute loss\n",
    "        # loss = criterion(batch_logits, batch_labels_ont_hot)\n",
    "        loss = model.get_loss(batch_logits, batch_labels_ont_hot)\n",
    "        # initial gradients\n",
    "        optimizer.zero_grad()\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update gradients\n",
    "        # optimizer.step()\n",
    "        for f in model.parameters():\n",
    "            f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "        print('loss: %-.5f, acc: %-.5f' %\n",
    "              (float(loss), float((torch.argmax(batch_logits, dim=-1) == batch_labels).float().mean())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
