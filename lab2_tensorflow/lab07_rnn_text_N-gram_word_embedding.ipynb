{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归神经网络 - Recurrent Neural Network\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Deep learning (also known as deep structured learning or hierarchical learning)\n",
    "is part of a broader family of machine learning methods based on learning data\n",
    "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
    "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
    "processing and communication patterns in a biological nervous system, such as neural\n",
    "coding that attempts to define a relationship between various stimuli and associated\n",
    "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
    "networks, deep belief networks and recurrent neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing,\n",
    "audio recognition, social network filtering, machine translation, bioinformatics\n",
    "and drug design,[5] where they have produced results comparable to and in some\n",
    "cases superior[6] to human experts.\n",
    "\"\"\".split()\n",
    "# from wikipedia https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "vocab = set(sentence)\n",
    "word2ind = {word: i for i, word in enumerate(vocab)}\n",
    "ind2word = dict(zip(word2ind.values(), word2ind.keys()))\n",
    "\n",
    "# hyper-parameter\n",
    "input_timesteps = 2\n",
    "output_timesteps = 1\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100\n",
    "\n",
    "hidden_size = 60\n",
    "layers_num = 2\n",
    "training_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = len(sentence) - input_timesteps\n",
    "x = [[word2ind[ch] for ch in sentence[i:i + input_timesteps]]\n",
    "     for i in xrange(len(sentence) - input_timesteps)]\n",
    "y = [[word2ind[sentence[i]]] for i in xrange(input_timesteps, len(sentence))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(dtype=tf.int32, shape=[None, input_timesteps])\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None, output_timesteps])\n",
    "\n",
    "onehot_encoding = lambda tensor: tf.one_hot(tensor, depth=vocab_size, axis=-1)\n",
    "output_tensor = onehot_encoding(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐资料：<br>\n",
    "[TensorFLow，RNN](https://www.tensorflow.org/tutorials/recurrent)<br>\n",
    "[TensorFlow，机器翻译](https://www.tensorflow.org/tutorials/seq2seq)<br>\n",
    "[TenorFlow，语音识别](https://www.tensorflow.org/tutorials/audio_recognition)<br>\n",
    "[Stanford，NLP课程](http://cs224d.stanford.edu/syllabus.html)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))\n",
    "embed = tf.nn.embedding_lookup(embedding_layer, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def RNN(x, num_hidden,\n",
    "        cell_type=rnn.BasicLSTMCell,\n",
    "        activation=tf.nn.relu,\n",
    "        dropout_prob=1.0,\n",
    "        num_layers=1):\n",
    "    assert cell_type in [rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell], \\\n",
    "        'RNN cell is wrong, must be in \"rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell\", but it is %s.' % (cell_type)\n",
    "    assert type(num_layers) == int and num_layers >= 1\n",
    "    assert 0.0 < dropout_prob <= 1.0\n",
    "\n",
    "    # RNN\n",
    "    def mRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    # BiRNN\n",
    "    def mBiRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    cell_fw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    cell_bw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "mLSTM = RNN(embed, hidden_size, dropout_prob=0.8, num_layers=2)\n",
    "mLSTM = tf.reshape(mLSTM, [-1, output_timesteps, input_timesteps * hidden_size * 2])\n",
    "fc1 = tf.layers.dense(inputs=mLSTM, units=vocab_size)\n",
    "y_ = fc1\n",
    "y_max = tf.argmax(y_, axis=-1)\n",
    "\n",
    "loss_op = tf.losses.softmax_cross_entropy(output_tensor, y_)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 10000\n",
      "Embeding Vector(10 dims) of learning: [ 0.00799441 -0.5405512   0.09135056  0.8176873  -0.12731647  0.8504865\n",
      " -0.45705009  0.66576886  0.06272578 -0.2356267 ]\n",
      "Epoch 1000 / 10000, training cost: 0.05490133\n",
      "Embeding Vector(10 dims) of learning: [-0.28414813 -0.5548082   0.01459709  0.69892734 -0.3096602   0.47404203\n",
      " -0.6738451   0.4596958   0.07354294 -0.18089756]\n",
      "Epoch 2000 / 10000, training cost: 0.9808509\n",
      "Embeding Vector(10 dims) of learning: [-0.44491723 -0.78584325 -0.43986446  0.67091227  0.28678086  0.44343904\n",
      " -0.25964335  0.49541366  0.02252071 -0.66784793]\n",
      "Epoch 3000 / 10000, training cost: 0.18120779\n",
      "Embeding Vector(10 dims) of learning: [-0.5225975  -1.0272251  -0.561625    1.1386011   0.22322932  0.33065888\n",
      " -0.6619925   0.5235973  -0.32603624 -0.69371986]\n",
      "Epoch 4000 / 10000, training cost: 0.6563275\n",
      "Embeding Vector(10 dims) of learning: [-0.41274878 -0.7972945  -0.5073091   1.0862406   0.00845267  0.0736497\n",
      " -1.0075927   0.30090532 -0.53805286 -0.56945497]\n",
      "Epoch 5000 / 10000, training cost: 0.51646304\n",
      "Embeding Vector(10 dims) of learning: [-0.60930866 -0.6619076  -0.42933404  1.1756237   0.32880884  0.2941958\n",
      " -0.75107586  0.4925977  -0.6175071  -0.54911596]\n",
      "Epoch 6000 / 10000, training cost: 0.044319715\n",
      "Embeding Vector(10 dims) of learning: [-0.5254916  -0.5860913  -0.5384417   1.3101562   0.34784257  0.3126354\n",
      " -0.92352283  0.5005893  -0.81989545 -0.3772571 ]\n",
      "Epoch 7000 / 10000, training cost: 0.05137785\n",
      "Embeding Vector(10 dims) of learning: [-0.20868205 -0.5217774  -0.5977173   1.4443862   0.44739565  0.12337033\n",
      " -0.40231854  0.5421753  -0.970046   -0.519707  ]\n",
      "Epoch 8000 / 10000, training cost: 2.5093603\n",
      "Embeding Vector(10 dims) of learning: [-0.16079603 -0.51088655 -0.5213838   1.4379044   0.39387384  0.19318551\n",
      " -0.5238088   0.7957776  -0.90852946 -0.40927756]\n",
      "Epoch 9000 / 10000, training cost: 0.046223376\n",
      "Embeding Vector(10 dims) of learning: [ 0.38333273 -0.7420678  -0.7629535   1.5918288   0.47023818  0.17661455\n",
      " -0.9025643   0.48121008 -1.2521913  -0.3152172 ]\n",
      "Epoch 10000 / 10000, training cost: 0.054046363\n",
      "Embeding Vector(10 dims) of learning: [ 0.24483342 -0.8414442  -0.70606595  1.3896894   0.5446279   0.06774496\n",
      " -1.0130334   0.61649203 -1.0230793  -0.20240648]\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "# 制定显存大小\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "keyword = 'learning'\n",
    "print('Epoch %s / %s' % (0, training_epochs))\n",
    "print('Embeding Vector(10 dims) of %s:' % (keyword), \\\n",
    "      session.run(embedding_layer[word2ind[keyword]])[:10])\n",
    "\n",
    "for i in xrange(1, 1 + training_epochs):\n",
    "    _, cost = session.run([optimizer, loss_op],\n",
    "                          feed_dict={X: x, Y: y})\n",
    "    if i % 1000 == 0:\n",
    "        print('Epoch %s / %s, training cost: %s' % (i, training_epochs, cost))\n",
    "        print('Embeding Vector(10 dims) of %s:' % (keyword), \\\n",
    "              session.run(embedding_layer[word2ind[keyword]])[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between these two sentences is 490\n",
      "\u001b[1;31;40mDeep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. Deep learning models are loosely related to information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics and drug design,[5] where they have produced results comparable to and in some cases superior[6] to human experts. \u001b[0m\n",
      "Deep learning architectures such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain. Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks and recurrent neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics and drug design,[5] where they have produced results comparable to and in some cases superior[6] to human experts. recognition, recurrent neural networks have been applied to fields including computer vision, speech recognition,\n"
     ]
    }
   ],
   "source": [
    "context_idxs = [word2ind['Deep'], word2ind['learning']]\n",
    "logue = context_idxs\n",
    "for i in xrange(data_num):\n",
    "    y_ = y_max.eval({X: [context_idxs], Y: y[:1]}, session)[0, 0]\n",
    "    logue.append(y_)\n",
    "    context_idxs = logue[-2:]\n",
    "\n",
    "sentence = ' '.join(sentence)\n",
    "pred_sentence = ' '.join([ind2word[i] for i in logue])\n",
    "\n",
    "import editdistance\n",
    "\n",
    "print('Distance between these two sentences is %s' % (editdistance.eval(sentence, pred_sentence)))\n",
    "print(\"\\033[1;31;40m%s \\033[0m\" % (sentence))\n",
    "print(pred_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节使用的语言模型是N-garm：利用前几个序列推导之后的一个序列。一般来说，前几个序列越长，模型的性能更好。另一种语言模型是CBOW，根据一个序列的前后几个序列来推导，中间序列。<br>\n",
    "word2vec是指，文本到向量。one-hot encoding和word embedding都属于word2vec。<br>\n",
    "[word embedding](http://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#sphx-glr-beginner-nlp-word-embeddings-tutorial-py)\n",
    "<br>\n",
    "word embedding可以加载离线训练好的结果，也可以在线训练。本案例中属于在线训练，因为我们用不到那么大的词汇库。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐资料：<br>\n",
    "[TensorFLow，RNN](https://www.tensorflow.org/tutorials/recurrent)<br>\n",
    "[TensorFlow，机器翻译](https://www.tensorflow.org/tutorials/seq2seq)<br>\n",
    "[TenorFlow，语音识别](https://www.tensorflow.org/tutorials/audio_recognition)<br>\n",
    "[Stanford，NLP课程](http://cs224d.stanford.edu/syllabus.html)<br>\n",
    "[word embedding--wikipedia](https://en.wikipedia.org/wiki/Word_embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
