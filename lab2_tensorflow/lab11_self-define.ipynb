{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Self-define自定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义loss函数\n",
    "`tf.reduce_mean(tf.losses.softmax_cross_entropy(y, y_))`和`tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_), axis=-1))`是等价的<br>\n",
    "tensorflow比较底层，loss函数定义较为自由"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义layer\n",
    "自定义网络可以通过`tf.Variable`实现，所有网络都是基于全相连网络实现（就是矩阵乘发。本质上是对特征向量维度的任意改变）\n",
    "```\n",
    "weights = tf.Variable(tf.random_normal([input_size, hidden_size]), name='kernel')\n",
    "bias = tf.Variable(tf.random_normal([hidden_size]), name='bias')\n",
    "linear1 = tf.matmul(x, weights) + bias\n",
    "```\n",
    "和<br>\n",
    "```\n",
    "tf.layers.dense(inputs=linear1, units=num_classes)\n",
    "```\n",
    "等价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义op\n",
    "一般的操作可以由tensorflow自带的运算组合而成<br>\n",
    "若需要自定义特殊运算，需要用到C/C++，难度较大。网络上存在这样的案例，有兴趣的读者可以尝试<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义optimizer\n",
    "tensorflow中对optimizer的定义较难。笔者以SGD的源码（部分，因为很多其他内容不常用）为例\n",
    "```\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.training import optimizer\n",
    "from tensorflow.python.training import training_ops\n",
    "\n",
    "class GradientDescentOptimizer(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate, use_locking=False, name=\"GradientDescent\"):\n",
    "        super(GradientDescentOptimizer, self).__init__(use_locking, name)\n",
    "        self._learning_rate = learning_rate\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        return training_ops.apply_gradient_descent(\n",
    "            var,\n",
    "            math_ops.cast(self._learning_rate_tensor, var.dtype.base_dtype),\n",
    "            grad,\n",
    "            use_locking=self._use_locking).op\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._learning_rate_tensor = ops.convert_to_tensor(self._learning_rate,\n",
    "                                                           name=\"learning_rate\")\n",
    "```\n",
    "\n",
    "`__init__`，`_apply_dense`，`_prepare`是三个最基础的必要函数，分别用于：定义，梯度计算和参数类型转换。\n",
    "在`_apply_dense`中，又将参数送到`apply_gradient_descent`中；而`apply_gradient_descent`有利用tensorflow写好的`_op_def_lib.apply_op(\"ApplyGradientDescent\", ...)`进行操作\n",
    "对于想自定义optimizer的用户来说，并不友好\n",
    "\n",
    "\n",
    "实际上，TF中的优化器已足够用户使用，无需额外定义。但是在某些情况下，我们需要修改网络的梯度，可以采用如下方式：<br>\n",
    "``\n",
    "optimizer_op = tf.train.AdamOptimizer()\n",
    "gradients, variables = zip(*optimizer_op.compute_gradients(loss_op))\n",
    "-# some operations on gradients, like\n",
    "gradients, _ = tf.clip_by_value(gradients, 5.0)\n",
    "train_op = optimizer_op.apply_gradients(zip(gradients, variables))\n",
    "``"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
