{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "递归神经网络 - Recurrent Neural Network\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "Deep learning (also known as deep structured learning or hierarchical learning)\n",
    "is part of a broader family of machine learning methods based on learning data\n",
    "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
    "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
    "processing and communication patterns in a biological nervous system, such as neural\n",
    "coding that attempts to define a relationship between various stimuli and associated\n",
    "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
    "networks, deep belief networks and recurrent neural networks have been applied to\n",
    "fields including computer vision, speech recognition, natural language processing,\n",
    "audio recognition, social network filtering, machine translation, bioinformatics\n",
    "and drug design,[5] where they have produced results comparable to and in some\n",
    "cases superior[6] to human experts.\n",
    "\"\"\"\n",
    "# from wikipedia https://en.wikipedia.org/wiki/Deep_learning\n",
    "\n",
    "chars = set(sentence)\n",
    "word2ind = {word: i for i, word in enumerate(chars)}\n",
    "ind2word = dict(zip(word2ind.values(), word2ind.keys()))\n",
    "\n",
    "# hyper-parameter\n",
    "input_timesteps = 2\n",
    "output_timesteps = 1\n",
    "num_classes = len(chars)\n",
    "hidden_size = 60\n",
    "layers_num = 2\n",
    "training_epochs = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = len(sentence) - input_timesteps\n",
    "x = [[word2ind[ch] for ch in sentence[i:i + input_timesteps]]\n",
    "     for i in xrange(len(sentence) - input_timesteps)]\n",
    "y = [[word2ind[sentence[i]]] for i in xrange(input_timesteps, len(sentence))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(dtype=tf.int32, shape=[None, input_timesteps])\n",
    "Y = tf.placeholder(dtype=tf.int32, shape=[None, output_timesteps])\n",
    "\n",
    "onehot_encoding = lambda tensor: tf.one_hot(tensor, depth=num_classes, axis=-1)\n",
    "input_tensor = onehot_encoding(X)\n",
    "output_tensor = onehot_encoding(Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN中的Dropout\n",
    "link: https://stackoverflow.com/questions/45917464/tensorflow-whats-the-difference-between-tf-nn-dropout-and-tf-contrib-rnn-dropo<br>\n",
    "tensorflow中有两种Dropout手段：<br>\n",
    "1.`tf.nn.droupout`：以上一个网络的输出的部分作为下一层网络的输入。适用于一切网络。<br>\n",
    "2.`tf.contrib.rnn.DropoutWrapper`：在RNN cell内部实现dropout，可以控制RNN网络的输入和输出dropout。只适用于RNN内部。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中创建多层RNN\n",
    "`tensorflow`中有2种方法可以实现多层RNN：<br>\n",
    "### 1.利用`rnn.MultiRNNCell`和`rnn.static_rnn`/`tf.nn.static_rnn`/`tf.nn.dynamic_rnn`的组合实现<br>\n",
    "#### i).`tf.nn.dynamic_rnn`<br>\n",
    "    不需要拆分<br>\n",
    "``\n",
    "cell = rnn.MultiRNNCell([rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "                             for _ in xrange(num_layers)])\n",
    "outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "``\n",
    "<br>\n",
    "#### ii).`rnn.static_rnn` <=> `tf.nn.static_rnn`<br>\n",
    "需要拆分<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "cell = rnn.MultiRNNCell([rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "                             for _ in xrange(num_layers)])\n",
    "outputs, state = tf.nn.static_rnn/rnn.static_rnn(cell, x, dtype=tf.float32)\n",
    "outputs = tf.concat(outputs, axis=-1)\n",
    "``<br>\n",
    "### 2.通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "#### i).`tf.nn.dynamic_rnn`<br>\n",
    "不需要拆分<br>\n",
    "``\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"rnn\"):\n",
    "        x, state = tf.nn.dynamic_rnn(\n",
    "            rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob),\n",
    "            inputs=x, dtype=tf.float32)\n",
    "outputs = x\n",
    "``<br>\n",
    "#### ii).`rnn.static_rnn` <=> `tf.nn.static_rnn`<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"rnn\"):\n",
    "        x, state = tf.nn.static_rnn(\n",
    "            rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob),\n",
    "            inputs=x, dtype=tf.float32)\n",
    "outputs = tf.concat(x, axis=-1)\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中创建多层RNN和多层BiRNN\n",
    "\n",
    "BiRNN不能像RNN那样灵活，需要控制输入和输出的流程，代码写起来比较冗长；不能直接使用for循环BiRNN，需要使用函数实现多层BiRNN<br>\n",
    "tensorflow中biRNN共有5个接口:<br>\n",
    "### 1.tensorflow.contrib.rnn.stack_bidirectional_rnn(cells_fw, cells_bw, ...)\n",
    "需要`tf.unstack`，将`[batch, timestep, length]`的`timestep`拆分为`list`<br>\n",
    "`cells_fw`, `cells_bw`必须为`list`，`list`的长度为RNN网络层数<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "cell_fw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "cell_bw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "outputs, state_fw, state_bw = rnn.stack_bidirectional_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "outputs = tf.stack(outputs, axis=1)\n",
    "``\n",
    "### 2.tensorflow.contrib.rnn.stack_bidirectional_dynamic_rnn\n",
    "不需要拆分<br>\n",
    "``\n",
    "cell_fw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "cell_bw = [rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "           for _ in xrange(num_layers)]\n",
    "outputs, state_fw, state_bw = rnn.stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "``\n",
    "### 3.tf.nn.bidirectional_dynamic_rnn\n",
    "不需要拆分；通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "``\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"bidirectional-rnn\"):\n",
    "        cell_fw = rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob)\n",
    "        cell_bw = rnn.DropoutWrapper(cell(units, activation=activation), output_keep_prob=dropout_prob)\n",
    "        x, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "    x = tf.concat(x, axis=-1)\n",
    "outputs = x\n",
    "``\n",
    "### 4.tensorflow.contrib.rnn.static_bidirectional_rnn = 5.tf.nn.static_bidirectional_rnn\n",
    "需要拆分；通过`tf.variable_scope`循环模拟多层RNN<br>\n",
    "``\n",
    "x = tf.unstack(x, timesteps, 1)\n",
    "for _ in xrange(num_layers):\n",
    "    with tf.variable_scope(None, default_name=\"bidirectional-rnn\"):\n",
    "        cell_fw = rnn.DropoutWrapper(cell(units, activation=activation))\n",
    "        cell_bw = rnn.DropoutWrapper(cell(units, activation=activation))\n",
    "        x, state_fw, state_bw = rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "outputs = tf.stack(x, axis=1)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import rnn\n",
    "\n",
    "def RNN(x, num_hidden,\n",
    "        cell_type=rnn.BasicLSTMCell,\n",
    "        activation=tf.nn.relu,\n",
    "        dropout_prob=1.0,\n",
    "        num_layers=1):\n",
    "    assert cell_type in [rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell], \\\n",
    "        'RNN cell is wrong, must be in \"rnn.BasicLSTMCell, rnn.BasicRNNCell, rnn.GRUCell\", but it is %s.' % (cell_type)\n",
    "    assert type(num_layers) == int and num_layers >= 1\n",
    "    assert 0.0 < dropout_prob <= 1.0\n",
    "\n",
    "    # RNN\n",
    "    def mRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    # BiRNN\n",
    "    def mBiRNN(x, units, cell=cell_type, activation=activation, num_layers=num_layers, dropout_prob=dropout_prob):\n",
    "        pass\n",
    "\n",
    "    cell_fw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    cell_bw = [rnn.DropoutWrapper(cell_type(num_hidden, activation=activation), output_keep_prob=dropout_prob) \\\n",
    "               for _ in xrange(num_layers)]\n",
    "    outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(cell_fw, cell_bw, inputs=x, dtype=tf.float32)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "mLSTM = RNN(input_tensor, hidden_size, dropout_prob=0.8, num_layers=2)\n",
    "mLSTM = tf.reshape(mLSTM, [-1, output_timesteps, input_timesteps * hidden_size * 2])\n",
    "fc1 = tf.layers.dense(inputs=mLSTM, units=num_classes)\n",
    "y_ = fc1\n",
    "y_max = tf.argmax(y_, axis=-1)\n",
    "\n",
    "loss_op = tf.losses.softmax_cross_entropy(output_tensor, y_)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 / 10000, training cost: 0.9628274\n",
      "Epoch 2000 / 10000, training cost: 0.9563312\n",
      "Epoch 3000 / 10000, training cost: 0.9443648\n",
      "Epoch 4000 / 10000, training cost: 0.93909156\n",
      "Epoch 5000 / 10000, training cost: 0.9439826\n",
      "Epoch 6000 / 10000, training cost: 0.939328\n",
      "Epoch 7000 / 10000, training cost: 0.93188685\n",
      "Epoch 8000 / 10000, training cost: 0.9366058\n",
      "Epoch 9000 / 10000, training cost: 0.9412674\n",
      "Epoch 10000 / 10000, training cost: 0.94335544\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "# 制定显存大小\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in xrange(1, 1 + training_epochs):\n",
    "    _, cost = session.run([optimizer, loss_op],\n",
    "                          feed_dict={X: x, Y: y})\n",
    "    if i % 1000 == 0:\n",
    "        print('Epoch %s / %s, training cost: %s' % (i, training_epochs, cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between these two sentences is 696\n",
      "\u001b[1;31;40m\n",
      "Deep learning (also known as deep structured learning or hierarchical learning)\n",
      "is part of a broader family of machine learning methods based on learning data\n",
      "representations, as opposed to task-specific algorithms. Learning can be supervised,\n",
      "semi-supervised or unsupervised. Deep learning models are loosely related to information\n",
      "processing and communication patterns in a biological nervous system, such as neural\n",
      "coding that attempts to define a relationship between various stimuli and associated\n",
      "neuronal responses in the brain. Deep learning architectures such as deep neural\n",
      "networks, deep belief networks and recurrent neural networks have been applied to\n",
      "fields including computer vision, speech recognition, natural language processing,\n",
      "audio recognition, social network filtering, machine translation, bioinformatics\n",
      "and drug design,[5] where they have produced results comparable to and in some\n",
      "cases superior[6] to human experts.\n",
      " \u001b[0m\n",
      "Deep learning as supervised to ing cogning compts supervised to known as supervised to have to ing deep learning mation, supervised to ing as supervised to ing deep learning as supervised to as supervised to deep learning compts supervised to have to known as supervised to ing deep learning deep learning as supervised to to ing deep learning cogning deep learning compts supervised to to to ing deep learning deep learning compts supervised to to to deep learning deep learning deep learning deep learning mation, supervised to to ing comparning compts supervised to to ing maching des supervised to deep learning comparning deep learning mation, supervised to have to ing deep learning mation, supervised to known as supervised to ing deep learning deep learning deep learning mation, supervised to known as supervised to have to ing as supervised to to ing deep learning deep learning deep learning deep learning deep learning compts supervis\n"
     ]
    }
   ],
   "source": [
    "context_idxs = [word2ind['D'], word2ind['e']]\n",
    "logue = context_idxs\n",
    "for i in xrange(data_num):\n",
    "    y_ = y_max.eval({X: [context_idxs], Y: y[:1]}, session)[0, 0]\n",
    "    logue.append(y_)\n",
    "    context_idxs = logue[-2:]\n",
    "\n",
    "sentence = ''.join(sentence)\n",
    "pred_sentence = ''.join([ind2word[i] for i in logue])\n",
    "\n",
    "import editdistance\n",
    "\n",
    "print('Distance between these two sentences is %s' % (editdistance.eval(sentence, pred_sentence)))\n",
    "print(\"\\033[1;31;40m%s \\033[0m\" % (sentence))\n",
    "print(pred_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN优点：<br>\n",
    "1.RNN适合处理关系序列。通常用于文本中。<br>\n",
    "2.为了加强RNN处理序列的能力，通常采用LSTM，GRU等高级cell。LSTM和GRU性能相近，但是GRU的参数更少，LSTM在论文中却仍然使用较多。因此，推荐读者准备一份LSTM/GRU的矢量图，和英文论文文档，便于以后使用。<br>\n",
    "*[理解LSTM和GRU](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "缺点：<br>\n",
    "1.参数较多，运行时间较长<br>\n",
    "\n",
    "推荐：<br>\n",
    "1.基于RNN+CTC的语音/图像识别：\n",
    "https://github.com/watsonyanghx/CNN_LSTM_CTC_Tensorflow<br>\n",
    "2.快速理解CTC：\n",
    "<br>\n",
    "3.翻译模型（Attention机制）：http://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐资料：<br>\n",
    "[TensorFLow，RNN](https://www.tensorflow.org/tutorials/recurrent)<br>\n",
    "[TensorFlow，机器翻译](https://www.tensorflow.org/tutorials/seq2seq)<br>\n",
    "[TenorFlow，语音识别](https://www.tensorflow.org/tutorials/audio_recognition)<br>\n",
    "[Stanford，NLP课程](http://cs224d.stanford.edu/syllabus.html)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
