{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python实现神经网络\n",
    "推荐资料：Michael Nielsen的[*Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级张量乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorMul(tensor1, tensor2, axis):\n",
    "    (a_extend, b_extend) = axis\n",
    "\n",
    "    def extendTensor(tensor, index):\n",
    "        shape = list(tensor.shape)\n",
    "        index = len(shape) if index == -1 else index\n",
    "        shape.insert(index, 1)\n",
    "        return np.reshape(tensor, newshape=shape)\n",
    "\n",
    "    tensor1 = extendTensor(tensor1, a_extend)\n",
    "    tensor2 = extendTensor(tensor2, b_extend)\n",
    "    batch_size = tensor1.shape[0]\n",
    "    tensor = np.asarray([np.dot(tensor1[i], tensor2[i]) for i in xrange(batch_size)])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全相连网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_size, output_size, bias_available=True, activation=None):\n",
    "        self.mapping_size = (input_size, output_size,)\n",
    "        self.input = None\n",
    "        # self.weight = np.random.randn(input_size, output_size)\n",
    "        self.weight = np.zeros(shape=(input_size, output_size))\n",
    "        self.bias_available = bias_available\n",
    "        if self.bias_available:\n",
    "            self.bias = np.zeros(shape=(output_size))\n",
    "        else:\n",
    "            pass\n",
    "        self.activation = activation\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.matmul(self.input, self.weight)\n",
    "        if self.bias_available:\n",
    "            self.output += self.bias\n",
    "        else:\n",
    "            pass\n",
    "        return self.activation(self.output)\n",
    "\n",
    "    def backprop(self, gard, optimizer):\n",
    "        # grad: gradient from behind layers\n",
    "        # self.activation(self.output, diff=False): difference between y_activation and y\n",
    "        dgrad_dy = gard * self.activation(self.output, diff=True)\n",
    "        # weight\n",
    "        delta_weight = tensorMul(self.input, dgrad_dy, axis=(-1, 1))\n",
    "        delta_weight = self.average(delta_weight)\n",
    "        delta_weight = optimizer(delta_weight)\n",
    "        # previous delta\n",
    "        next_grad = np.matmul(dgrad_dy, self.weight.T)\n",
    "        next_grad = self.average(next_grad)\n",
    "        # bias\n",
    "        if self.bias_available:\n",
    "            delta_bias = self.average(dgrad_dy)\n",
    "            delta_bias = optimizer(delta_bias)\n",
    "            return next_grad, delta_weight, delta_bias\n",
    "        else:\n",
    "            return next_grad, delta_weight, _\n",
    "\n",
    "    def updata(self, delta_weight, delta_bias):\n",
    "        self.weight += delta_weight\n",
    "        if self.bias_available:\n",
    "            self.bias += delta_bias\n",
    "\n",
    "    def average(self, x, axis=0):\n",
    "        return np.sum(x, axis=axis) / x.shape[0]\n",
    "\n",
    "    def size(self):\n",
    "        return self.mapping_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Activation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def relu(self, x, diff=False):\n",
    "        if diff:\n",
    "            pass\n",
    "        else:\n",
    "            return [i if i > 0 else 0 for i in x]\n",
    "\n",
    "    def linear(self, x, diff=False):\n",
    "        if diff:\n",
    "            return np.ones_like(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def sigmoid(self, x, diff=False):\n",
    "        if diff:\n",
    "            return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        else:\n",
    "            return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, diff=False):\n",
    "        if diff:\n",
    "            pass\n",
    "        else:\n",
    "            x_exp = np.exp(x)\n",
    "            x_exp_sum = np.sum(x_exp)\n",
    "            return [i / x_exp_sum for i in x_exp]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def SGD(self, grad, **kwargs):\n",
    "        return -self.learning_rate * grad\n",
    "\n",
    "    def Adam(self, grad, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def Adagrad(self, grad, **kwargs):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def MSE(self, y_true, y_pred, diff=False):\n",
    "        assert y_pred.shape == y_true.shape\n",
    "        if diff:\n",
    "            # cost = 1 / 2 * (y - y_) ** 2\n",
    "            # so, dcost = 2 * 1 / 2 * (y - y_) * (-1)\n",
    "            return y_pred - y_true\n",
    "        else:\n",
    "            return np.mean(np.sum(np.power(y_true - y_pred, 2), axis=-1))\n",
    "\n",
    "    def CrossEntropy(self, y_true, y_pred, diff=False):\n",
    "        if diff:\n",
    "            pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高级模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        self.layer = OrderedDict()\n",
    "        self.optimizer = Optimizer(learning_rate).SGD\n",
    "        self.cost_function = CostFunction()\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def __add__(self, layer, name=None):\n",
    "        if name == None:\n",
    "            self.layer['layer%02d' % (self.__len__())] = layer\n",
    "        else:\n",
    "            self.layer[name] = layer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layer.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def evaluate(self, y_pred, y_true):\n",
    "        y_pred = np.argmax(y_pred, axis=-1)\n",
    "        y_true = np.argmax(y_true, axis=-1)\n",
    "        return 1.0 * np.sum(y_true == y_pred) / y_pred.shape[0]\n",
    "\n",
    "    def backprop(self, grad):\n",
    "        for layer in reversed(self.layer.values()):\n",
    "            grad, delta_weight, delta_bias = layer.backprop(grad, self.optimizer)\n",
    "            layer.updata(delta_weight, delta_bias)\n",
    "\n",
    "    def train(self, x, y, episode=1, parenthesis=1):\n",
    "        history = {'cost': [], 'accuracy': []}\n",
    "        start = time.time()  # timer\n",
    "        for i in xrange(1, 1 + episode):\n",
    "            cost_value = 0.0\n",
    "            accuracy_value = 0.0\n",
    "            batch_episode = len(x) // self.batch_size\n",
    "            for j in xrange(batch_episode):\n",
    "                batch_x = x[j * self.batch_size:(j + 1) * self.batch_size]\n",
    "                batch_y = y[j * self.batch_size:(j + 1) * self.batch_size]\n",
    "                batch_y_ = self.forward(batch_x)\n",
    "                batch_cost_value = self.cost_function.MSE(y_true=batch_y, y_pred=batch_y_)\n",
    "                batch_accuracy = self.evaluate(y_pred=batch_y_, y_true=batch_y)\n",
    "                grad = self.cost_function.MSE(y_true=batch_y, y_pred=batch_y_, diff=True)\n",
    "                self.backprop(grad)\n",
    "                cost_value += batch_cost_value\n",
    "                accuracy_value += batch_accuracy\n",
    "                # print('Epoch %s / %s, sub-epoch %s / %s, cost is %s, accuracy is %s...' % \\\n",
    "                #       (i, episode, j, batch_episode, batch_cost_value, batch_accuracy))\n",
    "            cost_value /= batch_episode\n",
    "            accuracy_value /= batch_episode\n",
    "            if i % parenthesis == 0:\n",
    "                time_cost = time.time() - start\n",
    "                print('Epoch %s / %s, cost is %.5f, accuracy is %.5f, time cost is %.5f. (parenthesis=%s)' % \\\n",
    "                      (i, episode, cost_value, accuracy_value, time_cost, parenthesis))\n",
    "                start = time.time()  # timer\n",
    "            history['cost'].append(cost_value)\n",
    "            history['accuracy'].append(accuracy_value)\n",
    "        return history\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        for name, layer in self.layer.items():\n",
    "            input_size, output_size = layer.mapping_size\n",
    "            if layer.bias_available:\n",
    "                print('Linear Network', name, '\\n{input: [Batch, %s] × weight: [%s, %s] + bias: [%s]-> [Batch, %s]}' % \\\n",
    "                      (input_size, input_size, output_size, output_size, output_size))\n",
    "            else:\n",
    "                print('Linear Network', name, '\\n{input: [Batch, %s] × weight: [%s, %s] -> [Batch, %s]}' % \\\n",
    "                      (input_size, input_size, output_size, output_size))\n",
    "        print(\"=\" * 80 + \"\\n \")\n",
    "\n",
    "    def save(self, filename):\n",
    "        parameters = []\n",
    "        for layer in self.layer.values():\n",
    "            parameters.append([layer.weight, layer.bias])\n",
    "        np.save(file=filename, arr=parameters)\n",
    "\n",
    "    def load(self, filename):\n",
    "        parameters = np.load(file=filename)\n",
    "        for i, name in enumerate(self.layer.keys()):\n",
    "            weight, bias = parameters[i]\n",
    "            self.layer[name].weight = weight\n",
    "            self.layer[name].bias = bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "features = 28 ** 2\n",
    "hidden = 64\n",
    "num_classes = 10\n",
    "\n",
    "sigmoid = Activation().sigmoid\n",
    "linear = Activation().linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is 50000; test data is 10000\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "\n",
    "def load_data():\n",
    "    with gzip.open('data/mnist.pkl.gz', 'rb') as reader:\n",
    "        training_data, validation_data, test_data = cPickle.load(reader)\n",
    "        return (training_data, validation_data, test_data)\n",
    "\n",
    "def onehot_encoding(x, num_classes=num_classes):\n",
    "    size = [x.shape[0], num_classes]\n",
    "    y = np.zeros(size)\n",
    "    for i in xrange(size[0]):\n",
    "        y[i, x[i]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "(train_x, train_y), _, (test_x, test_y) = load_data()\n",
    "train_y = onehot_encoding(train_y)\n",
    "test_y = onehot_encoding(test_y)\n",
    "print('training data is %s; test data is %s' % (len(train_x), len(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Linear Network linear1 \n",
      "{input: [Batch, 784] × weight: [784, 10] + bias: [10]-> [Batch, 10]}\n",
      "================================================================================\n",
      " \n",
      "Epoch 10 / 100, cost is 0.31343, accuracy is 0.86226, time cost is 23.45271. (parenthesis=10)\n",
      "Epoch 20 / 100, cost is 0.26475, accuracy is 0.87766, time cost is 23.23998. (parenthesis=10)\n",
      "Epoch 30 / 100, cost is 0.24368, accuracy is 0.88416, time cost is 23.41176. (parenthesis=10)\n",
      "Epoch 40 / 100, cost is 0.23100, accuracy is 0.88854, time cost is 23.32661. (parenthesis=10)\n",
      "Epoch 50 / 100, cost is 0.22219, accuracy is 0.89186, time cost is 23.21651. (parenthesis=10)\n",
      "Epoch 60 / 100, cost is 0.21557, accuracy is 0.89376, time cost is 23.17856. (parenthesis=10)\n",
      "Epoch 70 / 100, cost is 0.21033, accuracy is 0.89578, time cost is 23.35666. (parenthesis=10)\n",
      "Epoch 80 / 100, cost is 0.20604, accuracy is 0.89766, time cost is 23.42308. (parenthesis=10)\n",
      "Epoch 90 / 100, cost is 0.20244, accuracy is 0.89878, time cost is 23.52733. (parenthesis=10)\n",
      "Epoch 100 / 100, cost is 0.19936, accuracy is 0.89998, time cost is 23.25529. (parenthesis=10)\n",
      "Accuracy: 0.9105\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "model = Sequential(learning_rate=1e-2)\n",
    "model.batch_size = 50\n",
    "model.__add__(Dense(features, num_classes, activation=sigmoid), name='linear1')\n",
    "\n",
    "# 模型结构输出\n",
    "model.summary()\n",
    "\n",
    "# 模型训练\n",
    "history = model.train(train_x, train_y, episode=100, parenthesis=10)\n",
    "\n",
    "# 保存/读取模型\n",
    "model.save(filename='model.npy')\n",
    "model.load(filename='model.npy')\n",
    "\n",
    "# 测试模型\n",
    "print('Accuracy:', model.evaluate(y_pred=model.predict(test_x), y_true=test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐资料：<br>\n",
    "[Stanford UFLDL课程，神经网络](http://deeplearning.stanford.edu/wiki/index.php/Neural_Networks)<br>\n",
    "[Stanford UFLDL课程，backprogation算法](http://deeplearning.stanford.edu/wiki/index.php/Backpropagation_Algorithm)<br>\n",
    "[Stanford UFLDL课程，backprogation算法](http://deeplearning.stanford.edu/wiki/index.php/Backpropagation_Algorithm)<br>\n",
    "[Stanford UFLDL课程，梯度检验与高级优化](http://deeplearning.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization)<br>\n",
    "[Micheal Nielsen, *Neural Networks and Deep Learning*](http://neuralnetworksanddeeplearning.com/index.html)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
